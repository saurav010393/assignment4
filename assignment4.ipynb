{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EQMrP2wF2mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0302b0e5-0e7e-4e2f-dc7f-bd8714065ac3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2NDNz1KHfnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the file must be loaded from drive:\n",
        "file_name=\"/content/drive/My Drive/Assignment_4\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafBSM8qIP4C",
        "colab_type": "text"
      },
      "source": [
        "#Loading important Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIDnza8ZIOgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdbkD5L4H4nW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ceb4a7e-b94e-48f5-9639-6afc761de09f"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
        "from keras import backend as k"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUHjbLdJPDK",
        "colab_type": "text"
      },
      "source": [
        "In this model we are using different types of Optimizers.Lets understand optimizer in brief:\n",
        "Def: Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.\n",
        "Various types are:\n",
        "1. SGD\n",
        "2. Nesterov Accelerated Gradient Descent\n",
        "3. RMSprop\n",
        "4. Adam\n",
        "5. Nadam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sONwzlwfLugw",
        "colab_type": "text"
      },
      "source": [
        "Lets start with SGD ie Stochastic gradient descent.\n",
        "Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties.\n",
        "\n",
        "Momentum in SGD: with momentum SGD remembers the update at each iteration, and determines the next update as a linear combination of the gradient and the previous update.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9J34to-H_Rq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "96b7a3d2-e3ae-4faf-eb87-591e2045d3ee"
      },
      "source": [
        "# initializing the CNN:\n",
        "cnn_classifier=Sequential()\n",
        "\n",
        "# Step1:Adding 1st layer of Convulation:\n",
        "cnn_classifier.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "##Step2: Pooling:\n",
        "cnn_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Repeat ie Adding Second Layer of Convulation:\n",
        "cnn_classifier.add(Conv2D(32,(3,3),activation='relu'))\n",
        "\n",
        "# Repeat ie Pooling:\n",
        "cnn_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Step3: Flattening:\n",
        "cnn_classifier.add(Flatten())\n",
        "\n",
        "#Step4: Full Connection:\n",
        "cnn_classifier.add(Dense(units=128,activation='relu'))\n",
        "cnn_classifier.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#Step5:Compiling The CNN:\n",
        "cnn_classifier.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "cnn_classifier.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 29, 29, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 813,217\n",
            "Trainable params: 813,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLNAYy-uMPpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "03df9f10-148e-42b7-afe8-e549b2c7f62d"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(64, 64),\n",
        "                                                   batch_size=8,\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=True)\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(64, 64),\n",
        "                                               batch_size=8,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fOrry2NUR0j",
        "colab_type": "text"
      },
      "source": [
        "In this part we will learn about callbacks in keras with its implementation.\n",
        "\n",
        "Def: A callback is a set of functions to be applied at given stages of the training procedure.\n",
        "We define and use a callback when we want to automate some tasks after every training/epoch that help us to control the training process. This includes stopping training when you reach a certain accuracy/loss score, saving model as a checkpoint after each successful epoch.\n",
        "\n",
        "EarlyStopping:\n",
        " One way to avoid overfitting is to terminate the process early. The EarlyStoppingfunction has various metrics/arguments that we can modify to set up when the training process should stop. \n",
        "\n",
        "Reduce learning rate:\n",
        "Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsfx-MLrQKBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs=callbacks.ModelCheckpoint('model_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=5,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.002, patience=3, verbose=1, min_delta=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUv73-AaVK9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub0pXKENTg-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "1630117e-4fe1-4b90-e548-c91583a8772a"
      },
      "source": [
        "history1=cnn_classifier.fit_generator(training_set,\n",
        "                             steps_per_epoch=1274//8,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=250,\n",
        "                             callbacks=[early_stopping,call_backs,red_on_plt])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "159/159 [==============================] - 616s 4s/step - loss: 0.6403 - accuracy: 0.6295 - val_loss: 0.7469 - val_accuracy: 0.3535\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.74695, saving model to model_callbacks.h5\n",
            "Epoch 2/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.5215 - accuracy: 0.7425 - val_loss: 0.8551 - val_accuracy: 0.3720\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.74695\n",
            "Epoch 3/20\n",
            "159/159 [==============================] - 11s 72ms/step - loss: 0.4730 - accuracy: 0.7694 - val_loss: 1.3758 - val_accuracy: 0.2920\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.74695\n",
            "Epoch 4/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.4245 - accuracy: 0.8073 - val_loss: 0.8068 - val_accuracy: 0.2525\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.74695\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.0000000949949025e-06.\n",
            "Epoch 5/20\n",
            "159/159 [==============================] - 11s 72ms/step - loss: 0.4162 - accuracy: 0.7899 - val_loss: 0.6070 - val_accuracy: 0.2785\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.74695 to 0.60697, saving model to model_callbacks.h5\n",
            "Epoch 6/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.3816 - accuracy: 0.8246 - val_loss: 1.3120 - val_accuracy: 0.2340\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.60697\n",
            "Epoch 7/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.3652 - accuracy: 0.8506 - val_loss: 1.9653 - val_accuracy: 0.2305\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.60697\n",
            "Epoch 8/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.3554 - accuracy: 0.8627 - val_loss: 1.2816 - val_accuracy: 0.2190\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.60697\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-09.\n",
            "Epoch 9/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.3594 - accuracy: 0.8633 - val_loss: 1.5796 - val_accuracy: 0.2180\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.60697\n",
            "Epoch 10/20\n",
            "159/159 [==============================] - 11s 71ms/step - loss: 0.3509 - accuracy: 0.8649 - val_loss: 2.0264 - val_accuracy: 0.2195\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.60697\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AddTR0GgBTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "5551a4be-b9c5-4d26-8fef-7abcdad0fd32"
      },
      "source": [
        "# initializing the CNN:\n",
        "cnn_classifier=Sequential()\n",
        "\n",
        "# Step1:Adding 1st layer of Convulation:\n",
        "cnn_classifier.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "##Step2: Pooling:\n",
        "cnn_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Repeat ie Adding Second Layer of Convulation:\n",
        "cnn_classifier.add(Conv2D(64,(3,3),activation='relu'))\n",
        "\n",
        "# Repeat ie Pooling:\n",
        "cnn_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Adding 3rd layer:\n",
        "cnn_classifier.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
        "cnn_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Step3: Flattening:\n",
        "cnn_classifier.add(Flatten())\n",
        "\n",
        "#Step4: Full Connection:\n",
        "cnn_classifier.add(Dense(units=128,activation='relu'))\n",
        "cnn_classifier.add(Dropout(0.10))\n",
        "cnn_classifier.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#Step5:Compiling The CNN:\n",
        "cnn_classifier.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "cnn_classifier.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 683,329\n",
            "Trainable params: 683,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pGZwCzWgSFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7ea9daa0-7151-422e-a9b7-ad365988099a"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(64, 64),\n",
        "                                                   batch_size=16,\n",
        "                                                  class_mode='binary')\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(64, 64),\n",
        "                                               batch_size=16,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci-HRXSigy02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs=callbacks.ModelCheckpoint('model_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=5,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=3, verbose=1, min_delta=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3vPFg48g8An",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4b7ec9a9-0c39-4900-af60-33c07715629c"
      },
      "source": [
        "history1=cnn_classifier.fit_generator(training_set,\n",
        "                             steps_per_epoch=500,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=500,\n",
        "                             callbacks=[early_stopping,call_backs,red_on_plt])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 29s 58ms/step - loss: 0.6399 - accuracy: 0.6384 - val_loss: 0.7114 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.60697\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 29s 57ms/step - loss: 0.4657 - accuracy: 0.7823 - val_loss: 1.3132 - val_accuracy: 0.2625\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.60697\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 29s 58ms/step - loss: 0.2966 - accuracy: 0.8732 - val_loss: 2.5225 - val_accuracy: 0.1562\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.60697\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 28s 57ms/step - loss: 0.2039 - accuracy: 0.9180 - val_loss: 3.9744 - val_accuracy: 0.1250\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.60697\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.0000000949949025e-06.\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 29s 57ms/step - loss: 0.2438 - accuracy: 0.8844 - val_loss: 4.3755 - val_accuracy: 0.1500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.60697\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 29s 58ms/step - loss: 0.1679 - accuracy: 0.9341 - val_loss: 3.1712 - val_accuracy: 0.1562\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.60697\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SH-Q2PbhInd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "43863387-1be2-48a7-a213-a102188d6c8f"
      },
      "source": [
        "cnn_classifier=Sequential()\n",
        "cnn_classifier.add(Conv2D(32, kernel_size=(3,3),input_shape=(200,200,3), activation='relu'))\n",
        "cnn_classifier.add(MaxPooling2D(2,2))\n",
        "cnn_classifier.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "cnn_classifier.add(MaxPooling2D(2,2))\n",
        "cnn_classifier.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
        "cnn_classifier.add(MaxPooling2D(2,2))\n",
        "cnn_classifier.add(Dropout(0.3))\n",
        "\n",
        "cnn_classifier.add(Flatten())\n",
        "\n",
        "cnn_classifier.add(Dense(128, activation='relu'))\n",
        "cnn_classifier.add(Dropout(0.25))\n",
        "cnn_classifier.add(Dense(1, activation='softmax'))\n",
        "\n",
        "cnn_classifier.compile(SGD(learning_rate=0.001, momentum=0.9), metrics=['accuracy'], loss='binary_crossentropy')\n",
        "\n",
        "cnn_classifier.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 198, 198, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 99, 99, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 97, 97, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 46, 46, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 23, 23, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 23, 23, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 67712)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               8667264   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 8,760,641\n",
            "Trainable params: 8,760,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HClTCGHIYhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7dbca0cf-2766-4160-cf89-2aa42321a08c"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(200, 200),\n",
        "                                                   batch_size=16,\n",
        "                                                  class_mode='binary')\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(200,200),\n",
        "                                               batch_size=16,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWtSwI1LHIGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs=callbacks.ModelCheckpoint('model_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=4,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=4, verbose=1, min_delta=0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJvlAKByHowC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "5e602d4a-efa6-425e-fbd0-686c7b596961"
      },
      "source": [
        "history1e=cnn_classifier.fit_generator(training_set,\n",
        "                             steps_per_epoch=1274//16,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=250,\n",
        "                             callbacks=[early_stopping,call_backs,red_on_plt])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 21s 260ms/step - loss: 7.7417 - accuracy: 0.4960 - val_loss: 10.4839 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 10.48385, saving model to model_callbacks.h5\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 20s 258ms/step - loss: 7.7300 - accuracy: 0.4944 - val_loss: 9.5308 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss improved from 10.48385 to 9.53077, saving model to model_callbacks.h5\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 20s 256ms/step - loss: 7.6594 - accuracy: 0.5000 - val_loss: 5.7185 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss improved from 9.53077 to 5.71846, saving model to model_callbacks.h5\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 20s 257ms/step - loss: 7.6738 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 5.71846\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 20s 258ms/step - loss: 7.6981 - accuracy: 0.4984 - val_loss: 11.4369 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 5.71846\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 20s 257ms/step - loss: 7.8243 - accuracy: 0.4897 - val_loss: 8.5777 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 5.71846\n",
            "Epoch 7/20\n",
            "79/79 [==============================] - 20s 256ms/step - loss: 7.7372 - accuracy: 0.4944 - val_loss: 6.6715 - val_accuracy: 0.5000\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 5.71846\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjBSFOXHIeJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " ##    My accuracy is not improving."
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Z_7ARnOKNA",
        "colab_type": "text"
      },
      "source": [
        "Nesterov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1C1m_J1OAeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "2811458e-b375-4508-86c1-67b992c660bd"
      },
      "source": [
        "# initializing the CNN:\n",
        "cnn_classifier1=Sequential()\n",
        "\n",
        "# Step1:Adding 1st layer of Convulation:\n",
        "cnn_classifier1.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "##Step2: Pooling:\n",
        "cnn_classifier1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Repeat ie Adding Second Layer of Convulation:\n",
        "cnn_classifier1.add(Conv2D(32,(3,3),activation='relu'))\n",
        "\n",
        "# Repeat ie Pooling:\n",
        "cnn_classifier1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Step3: Flattening:\n",
        "cnn_classifier1.add(Flatten())\n",
        "\n",
        "#Step4: Full Connection:\n",
        "cnn_classifier1.add(Dense(units=128,activation='relu'))\n",
        "cnn_classifier1.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#Step5:Compiling The CNN:\n",
        "cnn_classifier1.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9,nesterov='True'),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "cnn_classifier1.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 29, 29, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 813,217\n",
            "Trainable params: 813,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B0B1-KVSRWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "db6e5374-271b-4a39-d76a-6b52bb66b574"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(64, 64),\n",
        "                                                   batch_size=16,\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=True)\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(64, 64),\n",
        "                                               batch_size=16,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKseQeHGVMi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs=callbacks.ModelCheckpoint('model1_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=5,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=3, verbose=1, min_delta=0)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WsH9chPVNre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "5be25a26-2df9-4856-a9e8-bbad50cca1e2"
      },
      "source": [
        "history1=cnn_classifier1.fit_generator(training_set,\n",
        "                             steps_per_epoch=1274//16,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=250,\n",
        "                             callbacks=[early_stopping,call_backs,red_on_plt])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 18s 223ms/step - loss: 0.6718 - accuracy: 0.5866 - val_loss: 0.7119 - val_accuracy: 0.4125\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.71190, saving model to model1_callbacks.h5\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 18s 229ms/step - loss: 0.6115 - accuracy: 0.6820 - val_loss: 0.9360 - val_accuracy: 0.3562\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.71190\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 17s 216ms/step - loss: 0.5328 - accuracy: 0.7496 - val_loss: 1.1517 - val_accuracy: 0.2875\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.71190\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 18s 234ms/step - loss: 0.4672 - accuracy: 0.7774 - val_loss: 1.2786 - val_accuracy: 0.2812\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.71190\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 18s 222ms/step - loss: 0.4903 - accuracy: 0.7464 - val_loss: 1.7127 - val_accuracy: 0.3313\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.71190\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 18s 224ms/step - loss: 0.4735 - accuracy: 0.7547 - val_loss: 1.3334 - val_accuracy: 0.3250\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.71190\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYpJPBAyWsow",
        "colab_type": "text"
      },
      "source": [
        "RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Oz5yHDWugW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e336bb93-0d8d-412e-94e8-a172ef213302"
      },
      "source": [
        "# initializing the CNN:\n",
        "cnn_classifier2=Sequential()\n",
        "\n",
        "# Step1:Adding 1st layer of Convulation:\n",
        "cnn_classifier2.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "##Step2: Pooling:\n",
        "cnn_classifier2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Repeat ie Adding Second Layer of Convulation:\n",
        "cnn_classifier2.add(Conv2D(64,(3,3),activation='relu'))\n",
        "\n",
        "# Repeat ie Pooling:\n",
        "cnn_classifier2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Adding 3rd layer:\n",
        "cnn_classifier2.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
        "cnn_classifier2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Step3: Flattening:\n",
        "cnn_classifier2.add(Flatten())\n",
        "\n",
        "#Step4: Full Connection:\n",
        "cnn_classifier2.add(Dense(units=128,activation='relu'))\n",
        "cnn_classifier2.add(Dropout(0.50))\n",
        "cnn_classifier2.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#Step5:Compiling The CNN:\n",
        "cnn_classifier2.compile(RMSprop(), metrics=['accuracy'], loss='binary_crossentropy')\n",
        "\n",
        "cnn_classifier2.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 683,329\n",
            "Trainable params: 683,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfllsE_BYel2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6f9ddff1-dd13-4e7c-97cc-6e11bf1b7d60"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(64, 64),\n",
        "                                                   batch_size=16,\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=True)\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(64, 64),\n",
        "                                               batch_size=16,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQNybTu7Xv1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs=callbacks.ModelCheckpoint('model2_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=8,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=5, verbose=1, min_delta=0)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNMNLro5YWg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "566f8797-2ef2-4109-bb3e-72fc1368c9e7"
      },
      "source": [
        "history1=cnn_classifier1.fit_generator(training_set,\n",
        "                             steps_per_epoch=1274//16,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=1000,\n",
        "                             callbacks=[early_stopping,call_backs,red_on_plt])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 58s 737ms/step - loss: 0.6415 - accuracy: 0.6113 - val_loss: 0.7816 - val_accuracy: 0.4125\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.78158, saving model to model2_callbacks.h5\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 56s 712ms/step - loss: 0.6407 - accuracy: 0.6179 - val_loss: 0.7301 - val_accuracy: 0.4125\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.78158 to 0.73014, saving model to model2_callbacks.h5\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 57s 720ms/step - loss: 0.6421 - accuracy: 0.6142 - val_loss: 0.7287 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.73014 to 0.72872, saving model to model2_callbacks.h5\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 57s 720ms/step - loss: 0.6388 - accuracy: 0.6256 - val_loss: 0.7201 - val_accuracy: 0.4125\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.72872 to 0.72012, saving model to model2_callbacks.h5\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 58s 736ms/step - loss: 0.6411 - accuracy: 0.6264 - val_loss: 0.7301 - val_accuracy: 0.3938\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.72012\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 57s 719ms/step - loss: 0.6386 - accuracy: 0.6367 - val_loss: 0.7301 - val_accuracy: 0.3875\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.72012\n",
            "Epoch 7/20\n",
            "79/79 [==============================] - 56s 715ms/step - loss: 0.6399 - accuracy: 0.6359 - val_loss: 0.7800 - val_accuracy: 0.3812\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.72012\n",
            "Epoch 8/20\n",
            "79/79 [==============================] - 57s 722ms/step - loss: 0.6399 - accuracy: 0.6399 - val_loss: 0.7666 - val_accuracy: 0.3812\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.72012\n",
            "Epoch 9/20\n",
            "79/79 [==============================] - 57s 720ms/step - loss: 0.6391 - accuracy: 0.6447 - val_loss: 0.7154 - val_accuracy: 0.3812\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.72012 to 0.71538, saving model to model2_callbacks.h5\n",
            "Epoch 10/20\n",
            "79/79 [==============================] - 58s 738ms/step - loss: 0.6404 - accuracy: 0.6423 - val_loss: 0.7241 - val_accuracy: 0.3750\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.71538\n",
            "Epoch 11/20\n",
            "79/79 [==============================] - 57s 725ms/step - loss: 0.6363 - accuracy: 0.6653 - val_loss: 0.7318 - val_accuracy: 0.3750\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.71538\n",
            "Epoch 12/20\n",
            "79/79 [==============================] - 56s 707ms/step - loss: 0.6367 - accuracy: 0.6717 - val_loss: 0.7086 - val_accuracy: 0.3625\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.71538 to 0.70858, saving model to model2_callbacks.h5\n",
            "Epoch 13/20\n",
            "79/79 [==============================] - 56s 709ms/step - loss: 0.6408 - accuracy: 0.6638 - val_loss: 0.7324 - val_accuracy: 0.3562\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.70858\n",
            "Epoch 14/20\n",
            "79/79 [==============================] - 56s 709ms/step - loss: 0.6369 - accuracy: 0.6797 - val_loss: 0.7800 - val_accuracy: 0.3562\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70858\n",
            "Epoch 15/20\n",
            "79/79 [==============================] - 57s 716ms/step - loss: 0.6373 - accuracy: 0.6804 - val_loss: 0.7232 - val_accuracy: 0.3625\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70858\n",
            "Epoch 16/20\n",
            "79/79 [==============================] - 57s 719ms/step - loss: 0.6370 - accuracy: 0.6804 - val_loss: 0.6984 - val_accuracy: 0.3562\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70858 to 0.69836, saving model to model2_callbacks.h5\n",
            "Epoch 17/20\n",
            "79/79 [==============================] - 56s 714ms/step - loss: 0.6368 - accuracy: 0.6835 - val_loss: 0.7280 - val_accuracy: 0.3750\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69836\n",
            "Epoch 18/20\n",
            "79/79 [==============================] - 57s 727ms/step - loss: 0.6354 - accuracy: 0.6917 - val_loss: 0.7335 - val_accuracy: 0.3562\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69836\n",
            "Epoch 19/20\n",
            "79/79 [==============================] - 56s 711ms/step - loss: 0.6356 - accuracy: 0.6797 - val_loss: 0.7728 - val_accuracy: 0.3375\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69836\n",
            "Epoch 20/20\n",
            "79/79 [==============================] - 56s 710ms/step - loss: 0.6374 - accuracy: 0.6948 - val_loss: 0.7051 - val_accuracy: 0.3313\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6IqQGbYYuH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x2yqd_QZIEd",
        "colab_type": "text"
      },
      "source": [
        "Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-FKWz9ZPXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initializing the CNN:\n",
        "cnn_classifier2=Sequential()\n",
        "\n",
        "# Step1:Adding 1st layer of Convulation:\n",
        "cnn_classifier2.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "##Step2: Pooling:\n",
        "cnn_classifier2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Repeat ie Adding Second Layer of Convulation:\n",
        "cnn_classifier2.add(Conv2D(64,(3,3),activation='relu'))\n",
        "\n",
        "# Repeat ie Pooling:\n",
        "cnn_classifier2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Again Repeating ie Adding Third Layer of Convulation:\n",
        "cnn_classifier2.add(Conv2D(128,(3,3),activation='relu'))\n",
        "\n",
        "# Same with the this ie Pooling:\n",
        "cnn_classifier2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Step3: Flattening:\n",
        "cnn_classifier2.add(Flatten())\n",
        "\n",
        "#Step4: Full Connection:\n",
        "cnn_classifier2.add(Dense(units=128,activation='relu'))\n",
        "cnn_classifier2.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#Step5:Compiling The CNN:\n",
        "cnn_classifier2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agGK1LQFZisY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8ec88671-945b-460f-a889-6d0e8a47916e"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(64, 64),\n",
        "                                                   batch_size=16,\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=True)\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(64, 64),\n",
        "                                               batch_size=16,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRfZEwvtZwRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs1=callbacks.ModelCheckpoint('model2_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=8,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=5, verbose=1, min_delta=0)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZA20dH8doND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "7060422a-8cfa-439d-e810-a1be5ef1aa35"
      },
      "source": [
        "history1=cnn_classifier2.fit_generator(training_set,\n",
        "                             steps_per_epoch=1274//16,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=1000,\n",
        "                             callbacks=[early_stopping,call_backs1,red_on_plt])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 58s 733ms/step - loss: 0.5877 - accuracy: 0.6614 - val_loss: 0.9591 - val_accuracy: 0.2875\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.95909, saving model to model2_callbacks.h5\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 57s 721ms/step - loss: 0.3401 - accuracy: 0.8362 - val_loss: 4.0771 - val_accuracy: 0.2500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.95909\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 57s 722ms/step - loss: 0.2210 - accuracy: 0.9118 - val_loss: 4.1858 - val_accuracy: 0.1312\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.95909\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 56s 710ms/step - loss: 0.1133 - accuracy: 0.9634 - val_loss: 5.8719 - val_accuracy: 0.2313\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.95909\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 56s 705ms/step - loss: 0.1144 - accuracy: 0.9587 - val_loss: 8.5864 - val_accuracy: 0.1250\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.95909\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 57s 717ms/step - loss: 0.0589 - accuracy: 0.9801 - val_loss: 8.3909 - val_accuracy: 0.0938\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.95909\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
            "Epoch 7/20\n",
            "79/79 [==============================] - 57s 727ms/step - loss: 0.0258 - accuracy: 0.9968 - val_loss: 7.2730 - val_accuracy: 0.0938\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.95909\n",
            "Epoch 8/20\n",
            "79/79 [==============================] - 57s 723ms/step - loss: 0.0240 - accuracy: 0.9960 - val_loss: 7.9376 - val_accuracy: 0.0938\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.95909\n",
            "Epoch 9/20\n",
            "79/79 [==============================] - 56s 704ms/step - loss: 0.0244 - accuracy: 0.9968 - val_loss: 5.3083 - val_accuracy: 0.0938\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.95909\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gY6-hIheKgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trEEj8Agfgt2",
        "colab_type": "text"
      },
      "source": [
        "## Nadam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxUU7xU_fkYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initializing the CNN:\n",
        "cnn_classifier3=Sequential()\n",
        "\n",
        "# Step1:Adding 1st layer of Convulation:\n",
        "cnn_classifier3.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "##Step2: Pooling:\n",
        "cnn_classifier3.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Repeat ie Adding Second Layer of Convulation:\n",
        "cnn_classifier3.add(Conv2D(64,(3,3),activation='relu'))\n",
        "\n",
        "# Repeat ie Pooling:\n",
        "cnn_classifier3.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Again Repeating ie Adding Third Layer of Convulation:\n",
        "cnn_classifier3.add(Conv2D(128,(3,3),activation='relu'))\n",
        "\n",
        "# Same with the this ie Pooling:\n",
        "cnn_classifier3.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Step3: Flattening:\n",
        "cnn_classifier3.add(Flatten())\n",
        "\n",
        "#Step4: Full Connection:\n",
        "cnn_classifier3.add(Dense(units=128,activation='relu'))\n",
        "cnn_classifier3.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#Step5:Compiling The CNN:\n",
        "cnn_classifier3.compile(optimizer='Nadam',loss='binary_crossentropy',metrics=['accuracy'])\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc-uv4NkhB_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fe6edd3d-bf2c-4458-ff2c-dc7107039e5c"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set =  train_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/Train_data',\n",
        "                                                  target_size=(64, 64),\n",
        "                                                   batch_size=32,\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=True)\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/My Drive/Assignment_4/test_data',\n",
        "                                               target_size=(64, 64),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='binary')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1274 images belonging to 2 classes.\n",
            "Found 160 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTP2ayqbhLGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "call_backs2=callbacks.ModelCheckpoint('model2_callbacks.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss',\n",
        "                            min_delta=0,\n",
        "                            patience=8,\n",
        "                            verbose=1,\n",
        "                            restore_best_weights=True)\n",
        "red_on_plt=callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=5, verbose=1, min_delta=0)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOXvNPZ_hTOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "0dfdab02-9cb1-443d-ecbe-0011d9070746"
      },
      "source": [
        "history1=cnn_classifier3.fit_generator(training_set,\n",
        "                             steps_per_epoch=1274//32,\n",
        "                             epochs=20,\n",
        "                             validation_data=testing_set,\n",
        "                             validation_steps=200,\n",
        "                             callbacks=[early_stopping,call_backs2,red_on_plt])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.6519 - accuracy: 0.6208 - val_loss: 0.9635 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.96346, saving model to model2_callbacks.h5\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 24s 619ms/step - loss: 0.4449 - accuracy: 0.7681 - val_loss: 1.3985 - val_accuracy: 0.2625\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.96346\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 24s 620ms/step - loss: 0.3427 - accuracy: 0.8406 - val_loss: 2.8856 - val_accuracy: 0.2188\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.96346\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 26s 660ms/step - loss: 0.2202 - accuracy: 0.9147 - val_loss: 4.2963 - val_accuracy: 0.1875\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.96346\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 24s 620ms/step - loss: 0.1383 - accuracy: 0.9501 - val_loss: 5.9745 - val_accuracy: 0.1125\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.96346\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.2590 - accuracy: 0.9147 - val_loss: 3.5366 - val_accuracy: 0.1437\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.96346\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.0000000949949025e-06.\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 24s 615ms/step - loss: 0.1101 - accuracy: 0.9646 - val_loss: 5.1910 - val_accuracy: 0.1437\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.96346\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 24s 615ms/step - loss: 0.1131 - accuracy: 0.9597 - val_loss: 2.8060 - val_accuracy: 0.1437\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.96346\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 24s 610ms/step - loss: 0.1017 - accuracy: 0.9670 - val_loss: 3.5140 - val_accuracy: 0.1437\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.96346\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4dWj4x3hn5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}